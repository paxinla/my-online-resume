###  【Oct 2021 - Jan 2025】  A Cloud-Native Data Lakehouse Project

+  Data Lakehouse | Spark/Databricks

```
This project involved the establishment and continuous enhancement of a large-scale lakehouse system, providing robust data support for the company's research teams and various Web3 products.
```

My responsibilities in this project included:

1. Designing, planning, and implementing modern lakehouse and service facilities tailored to the company's business needs.
2. Analyzing and constructing efficient data models and ETL processes, continuously building and improving high-performance data services.
3. Participating in data research and analysis to meet product data requirements and support business decision-making.
4. Summarizing and maintaining technical documentation to promote the development of high-quality products.

The enterprise-level lakehouse system I was responsible for was built on a cloud-native solution using Databricks on AWS. It completely moved away from outdated Hadoop ecosystem tools, offering superior data management and processing capabilities. This solution significantly reduced the company's physical infrastructure and operational costs without increasing vendor lock-in risks. Using this lakehouse, I integrated and processed raw data from blockchain transactions (averaging around one million rows daily) and smart contracts, efficiently delivering high-quality data to the company's various Web3 data products and data science teams.

