###  【Nov 2015 - Nov 2018】  A data warehouse in the field of job seeking and employment

+ Data Warehouse | Hadoop | PostgreSQL

```  
This project serves as one of the company's data infrastructure components, providing stable and high-quality data support as well as technical support for massive data processing to data scientists and data mining engineers in the AI team.
``` 

My responsibilities in this project included:

1. Responsible for the construction of the core data warehouse and data marts in the Big Data platform, including development standards, dimensional modeling, and quality metric specifications.
2. Lead data development team members in processing data, including data cleaning, quantification, integration, storage, etc.
3. Plan and implement technical solutions for the Big Data platform.
4. Provide technical support to the AI algorithm team for processing massive amounts of data.


This project is a crucial component of the company's basic data infrastructure, integrating years of data in the field of job seeking and employment acquired by the data collection team. Its primary purpose is to provide datasets for data scientists and data mining engineers in the company's AI algorithm team, and to apply their generated models to the existing data (approximately 30TB) to obtain results for use in the company's data products. Additionally, it periodically generates employment statistics reports for colleges and universities in batches, which are utilized by the company's knockout product, "XX志愿".

In this project, I led the data development team to design and implement a data warehouse solution based on CDH and PostgreSQL, aligned with the company's data strategy objectives and the unique characteristics of employment-related data. We utilized various components such as Pentaho Data Integration, DataX, Hive, MapReduce, and Spark to build ETL processes and data pipelines. Additionally, we constructed a job scheduling system based on Apache Airflow. This centralized, automated scheduling and monitoring system replaced the previously scattered and reactive data transformation jobs that the company had been using. As a result, a full-scale computation job that originally took approximately one week to complete was reduced to just 1.5 days on the new platform.

